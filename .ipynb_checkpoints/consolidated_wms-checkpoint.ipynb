{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy import Selector\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "from weather import Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "search_text = \"Chennai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Yahoo Weather Information</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Yahoo', 29, 28, 29, 30, 31, 31)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    weather = Weather()\n",
    "    location = weather.lookup_by_location(search_text)\n",
    "    condition = location.condition()\n",
    "    yahoo_data_now = (\"Yahoo\", int((int(condition['temp'])-32)*float(0.5556)), \"\", \"\", \"\")\n",
    "\n",
    "    yahoo_url = \"https://query.yahooapis.com/v1/public/yql?\"\n",
    "    yql_query = \"select * from weather.forecast where woeid in (select woeid from geo.places(1) where text='{}')\".format(search_text)\n",
    "    yql_url = yahoo_url + urllib.urlencode({'q':yql_query}) + \"&format=json\"\n",
    "    result = urllib2.urlopen(yql_url).read()\n",
    "    data = json.loads(result)\n",
    "\n",
    "    week_data = data['query']['results'][\"channel\"][\"item\"][\"forecast\"]\n",
    "    day_1 = int((int(week_data[1]['high']) - 32) * 0.5556)\n",
    "    day_2 = int((int(week_data[2]['high']) - 32) * 0.5556)\n",
    "    day_3 = int((int(week_data[3]['high']) - 32) * 0.5556)\n",
    "    day_4 = int((int(week_data[4]['high']) - 32) * 0.5556)\n",
    "    day_5 = int((int(week_data[5]['high']) - 32) * 0.5556)\n",
    "    yahoo_data_now = (\"Yahoo\", int((int(condition['temp'])-32)*float(0.5556)), day_1, day_2, day_3, day_4, day_5)\n",
    "except:\n",
    "    yahoo_data_now = (\n",
    "        \"Yahoo\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "    )\n",
    "print yahoo_data_now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>The Weather Network</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.theweathernetwork.com/in/api/location/search?searchText={}\".format(search_text)\n",
    "req = requests.get(base_url)\n",
    "\n",
    "data = req.json()\n",
    "data_url = []\n",
    "if data:\n",
    "    for i in data:\n",
    "        if i[\"name\"] == search_text.title() and i[\"country\"] == \"India\":\n",
    "            each_url = \"https://www.theweathernetwork.com{}\".format(i[\"url\"])            \n",
    "            data_url.append({\"each_url\": each_url, \"code\": i[\"code\"]})\n",
    "\n",
    "try:            \n",
    "    if data_url:\n",
    "    \n",
    "        req_url = data_url[0]\n",
    "        current_weather_url = \"https://www.theweathernetwork.com/in/api/savedlocation/index/?placecodes={}\".format(req_url[\"code\"])\n",
    "        current_weather_req = requests.get(current_weather_url).json()\n",
    "        current_day_temp = current_weather_req[0][\"temperature\"]\n",
    "    \n",
    "        resp = urllib2.urlopen(req_url[\"each_url\"]).read()\n",
    "        soup = BeautifulSoup(resp, \"lxml\")\n",
    "    \n",
    "        current_data_day_1 = soup.find_all(\"div\", attrs={\"class\": \"day_1\"})\n",
    "        body = \"\"\"{}\"\"\".format(current_data_day_1[0])\n",
    "        selection_day_1 = Selector(text=body).xpath(\"//div\").extract()\n",
    "        temp_day_1 = re.findall(\"\\d+\", selection_day_1[5])[0]\n",
    "\n",
    "        current_data_day_2 = soup.find_all(\"div\", attrs={\"class\": \"day_2\"})\n",
    "        body = \"\"\"{}\"\"\".format(current_data_day_2[0])\n",
    "        selection_day_2 = Selector(text=body).xpath(\"//div\").extract()\n",
    "        temp_day_2 = re.findall(\"\\d+\", selection_day_2[5])[0]\n",
    "\n",
    "        current_data_day_3 = soup.find_all(\"div\", attrs={\"class\": \"day_3\"})\n",
    "        body = \"\"\"{}\"\"\".format(current_data_day_3[0])\n",
    "        selection_day_3 = Selector(text=body).xpath(\"//div\").extract()\n",
    "        temp_day_3 = re.findall(\"\\d+\", selection_day_3[5])[0]\n",
    "\n",
    "        current_data_day_4 = soup.find_all(\"div\", attrs={\"class\": \"day_4\"})\n",
    "        body = \"\"\"{}\"\"\".format(current_data_day_4[0])\n",
    "        selection_day_4 = Selector(text=body).xpath(\"//div\").extract()\n",
    "        temp_day_4 = re.findall(\"\\d+\", selection_day_4[5])[0]\n",
    "\n",
    "        current_data_day_5 = soup.find_all(\"div\", attrs={\"class\": \"day_5\"})\n",
    "        body = \"\"\"{}\"\"\".format(current_data_day_5[0])\n",
    "        selection_day_5 = Selector(text=body).xpath(\"//div\").extract()\n",
    "        temp_day_5 = re.findall(\"\\d+\", selection_day_5[5])[0]\n",
    "\n",
    "        weather_network_data_now = (\n",
    "            \"Weather Network\",\n",
    "            current_day_temp,\n",
    "            temp_day_1,\n",
    "            temp_day_2,\n",
    "            temp_day_3,\n",
    "            temp_day_4,\n",
    "            temp_day_5,\n",
    "        )\n",
    "    else:\n",
    "        weather_network_data_now = (\n",
    "            \"Weather Network\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "        )\n",
    "except:\n",
    "        weather_network_data_now = (\n",
    "            \"Weather Network\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "            \"NA\",\n",
    "        )\n",
    "    \n",
    "#url = req.json()[0]['url']\n",
    "#code = req.json()[0]['code']\n",
    "\n",
    "#ddict = defaultdict(list)\n",
    "\n",
    "#for i in data:\n",
    "#    if ddict[i[\"countrycode\"]]:\n",
    "#        ddict[i[\"countrycode\"]].append(i)\n",
    "#    else:\n",
    "#        ddict[i[\"countrycode\"]] = []\n",
    "#        ddict[i[\"countrycode\"]].append(i)\n",
    "\n",
    "#info_urls = []        \n",
    "#for k in ddict[\"IN\"]:\n",
    "#    if k[\"name\"] == search_text.title():\n",
    "#        info_urls.append(k['code'])\n",
    "        \n",
    "#print info_urls        \n",
    "#try:\n",
    "#    if info_urls:\n",
    "#        data = \"https://www.theweathernetwork.com/in/api/savedlocation/index/?placecodes={}\".format(info_urls[0])\n",
    "#        print data\n",
    "#        url_req = requests.get(data)\n",
    "#        print url_req.json()\n",
    "#        temperature = url_req.json()[0]['temperature']\n",
    "#        feels_like = url_req.json()[0]['feels_like']\n",
    "#        updated = url_req.json()[0]['updatetime']\n",
    "#        weather_network_data_now = (\"Weather Network\", str(temperature), \"\", \"\", \"\", \"\", \"\")\n",
    "#    else:\n",
    "#        weather_network_data_now = (\n",
    "#            \"Weather Network\",\n",
    "#            \"Not Found\",\n",
    "#            \"Not Found\",\n",
    "#            \"Not Found\",\n",
    "#            \"Not Found\",\n",
    "#            \"Not Found\",\n",
    "#            \"Not Found\",\n",
    "#        )\n",
    "#except:\n",
    "#    weather_network_data_now = (\n",
    "#        \"Weather Network\",\n",
    "#        \"Not Found\",\n",
    "#        \"Not Found\",\n",
    "#        \"Not Found\",\n",
    "#        \"Not Found\",\n",
    "#        \"Not Foundr\",\n",
    "#        \"Not Found\",\n",
    "#    )\n",
    "#print weather_network_data_now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> MSN Weather </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSN Weather', 32, 31, 31, 32, 'NA', 'NA')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    base_url = \"http://weather.service.msn.com/data.aspx?weasearchstr={}&culture=en-US&weadegreetype=F&src=outlook\".format(search_text)\n",
    "    req = requests.get(base_url)\n",
    "    root = ET.fromstring(req.content)\n",
    "    data = root.__dict__['_children'][0].__dict__['_children'][0].__dict__['attrib']\n",
    "    data_1 = root.__dict__['_children'][0].__dict__[\"_children\"][3].__dict__['attrib']['high']\n",
    "    data_2 = root.__dict__['_children'][0].__dict__[\"_children\"][4].__dict__['attrib']['high']\n",
    "    data_3 = root.__dict__['_children'][0].__dict__[\"_children\"][5].__dict__['attrib']['high']\n",
    "    msn_weather_now = (\n",
    "        \"MSN Weather\",\n",
    "        int((int(data['temperature'])-32)*float(0.5556)),\n",
    "        int((int(data_1)-32)*float(0.5556)),\n",
    "        int((int(data_2)-32)*float(0.5556)),\n",
    "        int((int(data_3)-32)*float(0.5556)),\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    msn_weather_now = (\n",
    "        \"MSN Weather\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "    )\n",
    "print msn_weather_now    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4> Accurate Weather </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accurate Weather', '33', '33', '34', '35', 'NA', 'NA')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:26: ScrapyDeprecationWarning: scrapy.selector.HtmlXPathSelector is deprecated, instantiate scrapy.Selector instead.\n",
      "/usr/local/lib/python2.7/dist-packages/parsel/selector.py:234: ScrapyDeprecationWarning: scrapy.selector.HtmlXPathSelector is deprecated, instantiate scrapy.Selector instead.\n",
      "  for x in result]\n"
     ]
    }
   ],
   "source": [
    "search_url = \"https://www.accuweather.com/en/search-locations\"\n",
    "search_data = {\"s\": search_text.title()}\n",
    "try:\n",
    "    req = requests.post(search_url, search_data)\n",
    "    resp = req.__dict__['_content']\n",
    "\n",
    "    soup = BeautifulSoup(resp, \"lxml\")\n",
    "    data_input_box = soup.find_all(\"input\", attrs={'id': 's'})\n",
    "\n",
    "    data_str = str(data_input_box[0])\n",
    "    data_url = re.findall(\"(https://www.accuweather.com/\\w+/\\w+/\\w+/[0-9]+/[\\w-]+/[0-9]+)\\\"\", data_str)[0]\n",
    "\n",
    "    base_url = data_url.replace(\"weather-forecast\", \"current-weather\")\n",
    "    base_link_url = base_url.replace(\"current-weather\", \"daily-weather-forecast\")\n",
    "    base_links = [\"{}?day={}\".format(base_link_url, i) for i in range(1, 6)]\n",
    "\n",
    "    req = urllib2.Request(base_url)\n",
    "    read_url = urllib2.urlopen(req)\n",
    "    response = read_url.read()\n",
    "    soup = BeautifulSoup(response, \"lxml\")\n",
    "    five_day_data = soup.find_all(\"li\", attrs={'data-href': base_links})\n",
    "\n",
    "    column_header = ('Day', 'Date', 'real_feel', 'precipitation', 'description')\n",
    "    data = []\n",
    "    for day in five_day_data:\n",
    "        hxs = HtmlXPathSelector(day).xpath(\"/html/body/p/text()\").extract()\n",
    "        doc = \"\"\"{}\"\"\".format(hxs[0].encode(\"utf-8\"))\n",
    "        each_day = re.sub(\"\\n+\",\n",
    "                          \",\",\n",
    "                          doc.decode(\"unicode_escape\").encode(\"ascii\", \"ignore\").replace(\"/\", \"\").replace(\"More\", \"\")\n",
    "                         )\n",
    "        data.append(filter(None, tuple(each_day.split(\",\")))) \n",
    "    accurate_weather_now = (\"Accurate Weather\", data[0][2], data[1][2], data[3][2], data[4][2], \"NA\", \"NA\")\n",
    "except:\n",
    "    accurate_weather_now = (\n",
    "        \"Accurate Weather\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "        \"NA\",\n",
    "    )\n",
    "print accurate_weather_now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>Weather Bug Weather</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# key = \"a723563832c75742\"\n",
    "# pip install pywu\n",
    "# base_url = \"https://www.wunderground.com/forecast/in/{}\".format(search_text)\n",
    "base_url = \"https://weather.weatherbug.com/api/localities/search?query={}\".format(search_text)\n",
    "slug_data = requests.get(base_url).json()[0]['SlugName']\n",
    "data_url = \"https://weather.weatherbug.com/weather-forecast/10-day-weather/{}\".format(slug_data)\n",
    "\n",
    "data_req = urllib2.urlopen(data_url)\n",
    "response = data_req.read()\n",
    "soup = BeautifulSoup(response, \"lxml\")\n",
    "day_data = soup.find_all(\"div\", attrs={\"class\": \"day-card__desktop\"})\n",
    "temp_value = []\n",
    "for each in day_data:\n",
    "    body = \"\"\"{}\"\"\".format(each)\n",
    "    body = body.decode(\"utf-8\").encode('ascii','ignore').replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "    select = Selector(text=body).xpath('//div[@class=\"temp\"]/text()').extract()\n",
    "    temp = int(round((int(select[0].strip()) - 32) * float(0.5556)))\n",
    "    temp_value.append(temp)\n",
    "weather_bug_data = (\"Weather Bug\", temp_value[0], temp_value[1], temp_value[2], temp_value[3], temp_value[4], temp_value[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h4>BBC Weather</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BBC Weather', 21, 21, 20, 20, 20, 'NA')\n"
     ]
    }
   ],
   "source": [
    "search_url = \"http://www.bbc.com/locator/default/en-GB/autocomplete.json?search={}&filter=international\".format(search_text)\n",
    "\n",
    "city_data = requests.get(search_url).json()\n",
    "try:\n",
    "    weather_data = []\n",
    "    if city_data:\n",
    "        city_data = city_data[0]\n",
    "        city_id = city_data['id']\n",
    "    \n",
    "        weather_url = \"http://www.bbc.com/weather/{}\".format(city_id)\n",
    "        data_req = urllib2.urlopen(weather_url)\n",
    "        response = data_req.read()\n",
    "        soup = BeautifulSoup(response, \"lxml\")\n",
    "        day_data = soup.find_all(\"td\", attrs={\"class\": \"min-temp\"})\n",
    "        for each in day_data:\n",
    "            body = \"\"\"{}\"\"\".format(each)\n",
    "            body = body.decode(\"utf-8\").encode('ascii','ignore')\n",
    "            select = Selector(text=body).xpath('//span[@data-unit=\"c\"]/text()').extract()\n",
    "            weather_data.append(int(select[0]))\n",
    "        bbc_weather_data = (\"BBC Weather\", weather_data[0], weather_data[1], weather_data[2], weather_data[3], weather_data[4], \"NA\")\n",
    "    else:\n",
    "        bbc_weather_data = (\"BBC Weather\", \"NA\", \"NA\", \"NA\", \"NA\", \"NA\", \"NA\")\n",
    "except Exception as e:\n",
    "    print e.message\n",
    "    bbc_weather_data = (\"BBC Weather\", \"NA\", \"NA\", \"NA\", \"NA\", \"NA\", \"NA\")\n",
    "print bbc_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The Weather Channel</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_text = \"bangalore\"\n",
    "search_url = \"https://dsx.weather.com/x/v2/web/loc/en_US/1/4/5/9/11/13/19/21/1000/1001/1003/us^/{}?format=json&pg=0%2C10\".format(search_text)\n",
    "chennai = \"https://dsx.weather.com/x/v2/web/loc/en_IN/1/4/5/9/11/13/19/21/1000/1001/1003/in^/chennai?api=7bb1c920-7027-4289-9c96-ae5e263980bc&format=json&pg=0%2C10\"\n",
    "bangalore = \"https://dsx.weather.com/x/v2/web/loc/en_IN/1/4/5/9/11/13/19/21/1000/1001/1003/in^/bangalore?api=7bb1c920-7027-4289-9c96-ae5e263980bc&format=json&pg=0%2C10\"\n",
    "madurai = \"https://dsx.weather.com/x/v2/web/loc/en_IN/1/4/5/9/11/13/19/21/1000/1001/1003/in^/madurai?api=7bb1c920-7027-4289-9c96-ae5e263980bc&format=json&pg=0%2C10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Consolidated Weather Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangalore\n",
      "\n",
      "                 Current Weather Day_1 Day_2 Day_3 Day_4 Day_5\n",
      "Sites                                                         \n",
      "Yahoo                         29    28    29    30    31    31\n",
      "MSN Weather                   32    31    31    32    NA    NA\n",
      "Weather Network               33    30    31    31    32    31\n",
      "Accurate Weather              33    33    34    35    NA    NA\n",
      "Weather Bug                   30    30    31    31    32    32\n",
      "BBC Weather                   21    21    20    20    20    NA\n"
     ]
    }
   ],
   "source": [
    "header = (\"Sites\", \"Current Weather\", \"Day_1\", \"Day_2\", \"Day_3\", \"Day_4\", \"Day_5\")\n",
    "dataset = [\n",
    "    yahoo_data_now,\n",
    "    msn_weather_now,\n",
    "    weather_network_data_now,\n",
    "    accurate_weather_now,\n",
    "    weather_bug_data,\n",
    "    bbc_weather_data,\n",
    "]\n",
    "df = pd.DataFrame(data=dataset, columns=header)\n",
    "print search_text.title()+\"\\n\"\n",
    "print df.set_index(\"Sites\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
